{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import h5py as h5\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import spikeinterface.full as si\n",
    "from sparsesorter.models.nss import NSS\n",
    "from sparsesorter.models.lca import LCA\n",
    "from sparsesorter.utils.metrics import SortingMetrics, compute_fscore_evolution\n",
    "from sparsesorter.utils.dataloader import build_dataloader\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "data_path = Path(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Spike Wafeforms:  (8303, 120)\n"
     ]
    }
   ],
   "source": [
    "ds_file = data_path / \"TS1.h5\"\n",
    "dataset, dataloader = build_dataloader(ds_file)\n",
    "print(\"Loaded Spike Wafeforms: \", dataset[\"wvs\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nss = NSS(\n",
    "    input_size=dataset[\"wvs\"].shape[1],\n",
    "    net_size=[120, 10],\n",
    "    threshold=0.04,\n",
    "    gamma=0.05,\n",
    "    lr=0.07,\n",
    "    bit_width=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 519/519 [00:35<00:00, 14.70it/s]\n"
     ]
    }
   ],
   "source": [
    "sorted_spikes, n_spikes = nss.fit_transform(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "GroundTruthComparison.__init__() got an unexpected keyword argument 'sampling_frequency'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m packet_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m----> 2\u001b[0m spike_processed, fscore_nss \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_fscore_evolution\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43msorted_spikes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpacket_size\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# plot\u001b[39;00m\n\u001b[1;32m      6\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots()\n",
      "File \u001b[0;32m~/LCA-SpikeSorting/sparsesorter/utils/metrics.py:98\u001b[0m, in \u001b[0;36mcompute_fscore_evolution\u001b[0;34m(sorted_spikes, dataset, packet_size)\u001b[0m\n\u001b[1;32m     90\u001b[0m mask_gtr \u001b[38;5;241m=\u001b[39m (gtr[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m spike_timing[i]) \u001b[38;5;241m&\u001b[39m (gtr[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m spike_timing[i \u001b[38;5;241m+\u001b[39m packet_size])\n\u001b[1;32m     91\u001b[0m gtsort_comp \u001b[38;5;241m=\u001b[39m SortingMetrics(\n\u001b[1;32m     92\u001b[0m     sorted_spikes[mask_pred],\n\u001b[1;32m     93\u001b[0m     spike_timing[mask_pred],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     96\u001b[0m     delta_time\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     97\u001b[0m )\n\u001b[0;32m---> 98\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mgtsort_comp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_fscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m score\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/LCA-SpikeSorting/sparsesorter/utils/metrics.py:63\u001b[0m, in \u001b[0;36mSortingMetrics.get_fscore\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_fscore\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 63\u001b[0m     sorting_perf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_sorting_perf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmatch_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhungarian\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m## compute F-score\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     matching_unit \u001b[38;5;241m=\u001b[39m sorting_perf\u001b[38;5;241m.\u001b[39mbest_match_12\u001b[38;5;241m.\u001b[39mto_numpy()\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n",
      "File \u001b[0;32m~/LCA-SpikeSorting/sparsesorter/utils/metrics.py:44\u001b[0m, in \u001b[0;36mSortingMetrics.get_sorting_perf\u001b[0;34m(self, match_mode)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m     exhaustive_gt, compute_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m sorting_perf \u001b[38;5;241m=\u001b[39m \u001b[43msi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompare_sorter_to_ground_truth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43msi_gt_raster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43msi_pred_raster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelta_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelta_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmatch_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmatch_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmatch_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampling_frequency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexhaustive_gt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexhaustive_gt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sorting_perf\n",
      "\u001b[0;31mTypeError\u001b[0m: GroundTruthComparison.__init__() got an unexpected keyword argument 'sampling_frequency'"
     ]
    }
   ],
   "source": [
    "packet_size = 100\n",
    "spike_processed, fscore_nss = compute_fscore_evolution(\n",
    "    sorted_spikes, dataset, packet_size\n",
    ")\n",
    "# plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(fscore_nss.T)\n",
    "ax.set_xlabel(f\"Number of processed packet ({packet_size} spikes)\")\n",
    "ax.set_ylabel(\"F1-score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"fs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute sorting F1-score evolution along the dataset\n",
    "sorting_metrics = SortingMetrics(dataset[\"spike_clusters\"], sorted_spikes, n_spikes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute F1-score\n",
    "gtsort_comp = GTSortingComparison(\n",
    "    label_nss,\n",
    "    dataset[\"raster\"],\n",
    "    dataset[\"gt_raster\"],\n",
    "    fs,\n",
    "    delta_time=2,\n",
    ")  # train evalution f-score\n",
    "fscore_nss = gtsort_comp.get_fscore().round(4)\n",
    "print(f\"F1s NSS: {fscore_nss.mean()*100:.2f}% | {fscore_nss*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(\n",
    "    file_path, dst_name, model_name, N, trial, nspikes, spike_processed, snr, fscore_nss\n",
    "):\n",
    "    if os.path.exists(file_path):\n",
    "        previous_perf = pd.read_pickle(file_path)\n",
    "    else:\n",
    "        previous_perf = pd.DataFrame(\n",
    "            columns=[\n",
    "                \"model\",\n",
    "                \"dataset_name\",\n",
    "                \"N\",\n",
    "                \"nspikes\",\n",
    "                \"trial\",\n",
    "                \"spike_processed\",\n",
    "                \"snr\",\n",
    "                \"fscore\",\n",
    "            ]\n",
    "        )\n",
    "    # first create pd Dataframe with time_step, snr, fscore\n",
    "    perf = pd.DataFrame(columns=[\"spike_processed\", \"snr\", \"fscore\"])\n",
    "    for ni in range(snr.size):\n",
    "        perf = pd.concat(\n",
    "            [\n",
    "                perf,\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                        \"spike_processed\": spike_processed,\n",
    "                        \"snr\": np.repeat(snr[ni], spike_processed.size),\n",
    "                        \"fscore\": fscore_nss[ni],\n",
    "                    }\n",
    "                ),\n",
    "            ],\n",
    "            axis=0,\n",
    "        )\n",
    "    perf.insert(0, \"model\", model_name)\n",
    "    perf.insert(1, \"dataset_name\", dst_name)\n",
    "    perf.insert(2, \"N\", N)\n",
    "    perf.insert(3, \"nspikes\", nspikes)\n",
    "    perf.insert(4, \"trial\", trial)\n",
    "    res_df = pd.concat([previous_perf, perf], axis=0)\n",
    "    res_df.to_pickle(file_path)\n",
    "\n",
    "\n",
    "def run_nss(dataloader, seed, N=2, model=\"TDQ\", ths=[0.07, 0.07], n_atoms=[120, 10]):\n",
    "    params_nss = {\n",
    "        \"n_atoms1\": n_atoms[0],\n",
    "        \"n_atoms2\": n_atoms[1],\n",
    "        \"D1_positive\": False,\n",
    "        \"D2_positive\": True,\n",
    "        \"th1\": ths[0],\n",
    "        \"th2\": ths[1],\n",
    "        \"fs\": fs,\n",
    "        \"tau\": 2e-3,\n",
    "        \"iters\": 200,\n",
    "        \"lr\": 0.07,\n",
    "        \"n_model\": model,\n",
    "        \"q\": 2**N - 1,\n",
    "        \"seed\": seed,\n",
    "    }\n",
    "    params_nss[\"gamma\"] = 1 / params_nss[\"fs\"] / params_nss[\"tau\"]\n",
    "\n",
    "    ## init lca1\n",
    "    lca1 = LCA(\n",
    "        input_size=next(iter(dataloader))[0].shape[1],\n",
    "        gamma=params_nss[\"gamma\"],\n",
    "        threshold=params_nss[\"th1\"],\n",
    "        n_atoms=params_nss[\"n_atoms1\"],\n",
    "        lr=params_nss[\"lr\"],\n",
    "        neuron_model=params_nss[\"n_model\"],\n",
    "        q=params_nss[\"q\"],\n",
    "        D_positive=params_nss[\"D1_positive\"],\n",
    "        seed=params_nss[\"seed\"],\n",
    "    )\n",
    "    lca2 = LCA(\n",
    "        input_size=params_nss[\"n_atoms1\"],\n",
    "        gamma=params_nss[\"gamma\"],\n",
    "        threshold=params_nss[\"th2\"],\n",
    "        n_atoms=params_nss[\"n_atoms2\"],\n",
    "        lr=params_nss[\"lr\"],\n",
    "        neuron_model=params_nss[\"n_model\"],\n",
    "        q=params_nss[\"q\"],\n",
    "        D_positive=params_nss[\"D2_positive\"],\n",
    "        seed=params_nss[\"seed\"],\n",
    "    )\n",
    "    nss = NSS_online(lca1, lca2, params_nss[\"iters\"], scale_factor=0.8)\n",
    "\n",
    "    # training NSS\n",
    "    nss_out = []\n",
    "    n_spikes = []\n",
    "    for _, (bi, ri) in enumerate(tqdm(dataloader)):\n",
    "        if int(ri[-1]) / fs > 120:\n",
    "            nss.lca1.lr, nss.lca2.lr = 0.01, 0.01\n",
    "            nss.iters = 64\n",
    "            n_spikes.append(nss.lca1.n_spikes + nss.lca2.n_spikes)\n",
    "        nss(bi)\n",
    "        nss_out.append(nss.lca2.decoded_out.numpy())\n",
    "\n",
    "    nss_out = np.concatenate(nss_out, axis=0)\n",
    "    n_spikes = np.concatenate(n_spikes, axis=0)\n",
    "    labels = np.argmax(nss_out, axis=1).astype(int)\n",
    "    # print(f\"scale factor : {nss.scale_factor}\")\n",
    "    return labels, n_spikes, nss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIG3 : Recording Trace, Ground Truth Raster and Inferred raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get trace and waveforms\n",
    "# rec_f = si.load_extractor(\n",
    "#     \"data/hc1/d533101_extra\"\n",
    "# )  # \"data/tetrode/tetrode49_n5_recording\")\n",
    "# mads = si.get_noise_levels(rec_f, return_scaled=False)\n",
    "# detection_th = 5 * mads\n",
    "\n",
    "ds_file = \"data/hc1/hc1_d533101_dth5_tmax240_noburst.h5\"\n",
    "dataset, dataloader = load_dataset(ds_file, tmax=240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run nss and get\n",
    "n_atoms, ths = [120, 10], [0.03, 0.03]\n",
    "params_nss = {\n",
    "    \"n_atoms1\": n_atoms[0],\n",
    "    \"n_atoms2\": n_atoms[1],\n",
    "    \"D1_positive\": False,\n",
    "    \"D2_positive\": True,\n",
    "    \"th1\": ths[0],\n",
    "    \"th2\": ths[1],\n",
    "    \"fs\": fs,\n",
    "    \"tau\": 2e-3,\n",
    "    \"iters\": 200,\n",
    "    \"lr\": 0.07,\n",
    "    \"n_model\": \"TDQ\",\n",
    "    \"q\": 2**2 - 1,\n",
    "    \"seed\": 0,\n",
    "}\n",
    "params_nss[\"gamma\"] = 1 / params_nss[\"fs\"] / params_nss[\"tau\"]\n",
    "\n",
    "## init lca1\n",
    "lca1 = LCA1iter(\n",
    "    input_size=next(iter(dataloader))[0].shape[1],\n",
    "    gamma=params_nss[\"gamma\"],\n",
    "    threshold=params_nss[\"th1\"],\n",
    "    n_atoms=params_nss[\"n_atoms1\"],\n",
    "    lr=params_nss[\"lr\"],\n",
    "    neuron_model=params_nss[\"n_model\"],\n",
    "    q=params_nss[\"q\"],\n",
    "    D_positive=params_nss[\"D1_positive\"],\n",
    "    seed=params_nss[\"seed\"],\n",
    ")\n",
    "lca2 = LCA1iter(\n",
    "    input_size=params_nss[\"n_atoms1\"],\n",
    "    gamma=params_nss[\"gamma\"],\n",
    "    threshold=params_nss[\"th2\"],\n",
    "    n_atoms=params_nss[\"n_atoms2\"],\n",
    "    lr=params_nss[\"lr\"],\n",
    "    neuron_model=params_nss[\"n_model\"],\n",
    "    q=params_nss[\"q\"],\n",
    "    D_positive=params_nss[\"D2_positive\"],\n",
    "    seed=params_nss[\"seed\"],\n",
    ")\n",
    "nss = NSS_online(lca1, lca2, params_nss[\"iters\"])  # , scale_factor=0.5\n",
    "\n",
    "# training NSS\n",
    "nss_out = []\n",
    "n_spikes = []\n",
    "for _, (bi, ri) in enumerate(tqdm(dataloader)):\n",
    "    if int(ri[-1]) / fs > 90:  # reduce lr after 60s\n",
    "        nss.lca1.lr, nss.lca2.lr = 0.01, 0.01\n",
    "        nss.iters = 50\n",
    "    nss(bi)\n",
    "    nss_out.append(nss.lca2.decoded_out.numpy())\n",
    "\n",
    "nss_out = np.concatenate(nss_out, axis=0)\n",
    "labels = np.argmax(nss_out, axis=1).astype(int)\n",
    "gtsort_comp = GTSortingComparison(\n",
    "    labels,\n",
    "    dataset[\"raster\"],\n",
    "    dataset[\"gt_raster\"],\n",
    "    fs,\n",
    "    delta_time=1,\n",
    ")\n",
    "sorting_perf = gtsort_comp.get_sorting_perf(match_mode=\"hungarian\")\n",
    "best_match_12 = sorting_perf.best_match_12\n",
    "natoms_out = nss_out.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# from matplotlib import gridspec\n",
    "# plt.style.use(\"seaborn-v0_8-paper\")\n",
    "# fig = plt.figure(figsize=(6, 7), dpi=150)\n",
    "\n",
    "# time_range = (75, 80)  # time range in s\n",
    "# t = np.arange(time_range[0], time_range[1], 1 / fs)\n",
    "# peaks_train = dataset[\"raster\"]\n",
    "# mask_trange = (peaks_train >= time_range[0] * fs) & (peaks_train < time_range[1] * fs)\n",
    "# peaks = peaks_train[mask_trange]\n",
    "# trace = rec_f.get_traces()[int(time_range[0] * fs) : int(time_range[1] * fs), :]\n",
    "# min_trace, max_trace = np.min(trace)-10, np.max(trace)+10\n",
    "# nss_out_trange = nss_out[mask_trange]\n",
    "\n",
    "# gs = gridspec.GridSpec(6, 1, height_ratios=[0.1,0.1,0.1,0.1, 0.15, 0.4])\n",
    "# # create a subplot of 4 rows and 1 column with gs[0:3]\n",
    "# ax03 = [plt.subplot(gs[i]) for i in range(4)]\n",
    "# ax1 = plt.subplot(gs[4])\n",
    "# ax2 = plt.subplot(gs[5])\n",
    "# for ch in range(nchan):\n",
    "#     trace_ch = rec_f.get_traces()[int(time_range[0] * fs) : int(time_range[1] * fs), ch]\n",
    "#     ax03[ch].plot(t, trace_ch, c=\"k\", alpha=0.5)  # trace\n",
    "#     ax03[ch].axhline(-detection_th[ch], c=\"k\", linestyle=\"--\")  # detection threshold\n",
    "#     ax03[ch].spines[[\"bottom\", \"top\", \"right\"]].set_visible(False)\n",
    "#     ax03[ch].set_xticks([])\n",
    "#     ax03[ch].set_ylim(min_trace, max_trace)\n",
    "#     ax03[ch].set_ylabel(f\"Ch{ch+1}\")\n",
    "#     for p in peaks:\n",
    "#         win_width = 3 * fs // 1000\n",
    "#         trace_window = rec_f.get_traces()[p - int(0.4 * win_width) : p + int(0.6 * win_width),:]\n",
    "#         p -= int(time_range[0] * fs)\n",
    "#         max_chan = np.argmax(np.max(np.abs(trace_window), axis=0))\n",
    "#         if max_chan == ch:\n",
    "#             t_window = t[p - int(0.4 * win_width) : p + int(0.6 * win_width)]\n",
    "#             ax03[ch].plot(t_window, trace_window[:,ch], c='k')\n",
    "\n",
    "# peaks = peaks - int(time_range[0] * fs)\n",
    "# # plot gt_raster_train on the same time range\n",
    "# gtr_train = dataset[\"gt_raster\"]\n",
    "# gtr_train = gtr_train[\n",
    "#     :, (gtr_train[0] >= time_range[0] * fs) & (gtr_train[0] < time_range[1] * fs)\n",
    "# ]\n",
    "# c_unit = plt.cm.Set1(np.linspace(0, 1, 9))\n",
    "# for i in range(nneurons):\n",
    "#     idx = np.where(gtr_train[1] == i)[0]\n",
    "#     ax1.vlines(gtr_train[0][idx] / fs, i - 0.4, i + 0.4, color=c_unit[i], lw=0.8)\n",
    "# ax1.set_ylabel(\"Ground Truth\")\n",
    "# ax1.spines[[\"bottom\", \"top\", \"right\"]].set_visible(False)\n",
    "# ax1.set_yticks(np.arange(0, nneurons, 1))\n",
    "# # ax1.set_yticklabels([])\n",
    "# ax1.set_xticks([])\n",
    "\n",
    "# # plot pred_raster on the same time range\n",
    "# c_out_nss = plt.cm.tab10(np.linspace(0, 1, 10))\n",
    "# for i, out_i in enumerate(nss_out[mask_trange]):\n",
    "#     peak_i = peaks[i]\n",
    "#     atom_active = len(out_i) - np.argmax(out_i)\n",
    "#     ax2.vlines(peak_i / fs, atom_active - 0.4, atom_active + 0.4, color=\"k\", lw=0.8)\n",
    "# ax2.set_ylabel(\"Inferred Raster - NSS\")\n",
    "# ax2.spines[[\"top\", \"right\"]].set_visible(False)\n",
    "# # set y-ticks at every 1 unit but label at every 2 units\n",
    "# ax2.set_yticks(np.arange(1, natoms_out+1, 1))\n",
    "# ax2.set_yticklabels([])\n",
    "# ax2.tick_params(axis=\"y\", which=\"major\")\n",
    "# ax2.set_yticks(np.arange(1, natoms_out+1,2), minor=True)\n",
    "# ax2.set_yticklabels(np.arange(1, natoms_out+1,2), minor=True)\n",
    "# ax2.tick_params(axis=\"y\", which=\"minor\", labelsize=8)\n",
    "# ax2.set_xticks(np.arange(t[0], t[-1] + 1, 1) - t[0])\n",
    "# ax2.set_xticklabels(np.arange(t[0], t[-1] + 1, 1, dtype=int))\n",
    "# ax2.set_xlabel(\"Time (s)\")\n",
    "\n",
    "# # draw a rectangle around the vlines of the atom 0\n",
    "# for ni in range(nneurons):\n",
    "#     pos_y = nss_out.shape[1] - best_match_12[ni] - 0.4\n",
    "#     len_x = time_range[1] - time_range[0]\n",
    "#     ax2.add_patch(\n",
    "#         plt.Rectangle(\n",
    "#             (0, pos_y), len_x + 0.1, 0.8, fill=False, edgecolor=c_unit[ni], lw=1\n",
    "#         )\n",
    "#     )\n",
    "# plt.savefig(\"figures/fig2bis_hc1_trace&raster.svg\", format=\"svg\", dpi=150, bbox_inches=\"tight\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Evolution of NSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hyperparameters\n",
    "# seed = 0\n",
    "# nneurons = 5\n",
    "# ds = 209  # [49, 62, 71, 209]\n",
    "# data = f\"data/tetrode/tetrode{ds}_n5_static.h5\"\n",
    "# # data = \"data/hc1/hc1_d533101_dth5_tmax240_noburst.h5\"\n",
    "# dataset, dataloader = load_dataset(data, tmax=240)\n",
    "# print(f\"snr = {dataset['snr']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run NSS\n",
    "label_nss, nss_out, nss = run_nss(\n",
    "    dataloader, seed, N=8, model=\"TDQ\", ths=[0.04, 0.04], n_atoms=[120, 10]\n",
    ")\n",
    "gtsort_comp = GTSortingComparison(\n",
    "    label_nss,\n",
    "    dataset[\"raster\"],\n",
    "    dataset[\"gt_raster\"],\n",
    "    fs,\n",
    "    delta_time=2,\n",
    ")  # train evalution f-score\n",
    "fscore_nss = gtsort_comp.get_fscore().round(4)\n",
    "print(f\"F1s NSS: {fscore_nss.mean()*100:.2f}% | {fscore_nss*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save NSS params + wvs + gt_raster and cpu labels for loihi2 runs:\n",
    "# with h5.File(\"data/loihi2/tetrode209_trained_nss.h5\", \"w\") as f:\n",
    "#     f.create_dataset(\"D1\", data=nss.lca1.D.numpy())\n",
    "#     f.create_dataset(\"D2\", data=nss.lca2.D.numpy())\n",
    "#     f.create_dataset(\"wvs\", data=dataset[\"wvs\"])\n",
    "#     f.create_dataset(\"gt_raster\", data=dataset[\"gt_raster\"])\n",
    "#     f.create_dataset(\"peaks_idx\", data=dataset[\"raster\"])\n",
    "#     f.create_dataset(\"label_test\", data=label_nss)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # compute the fscore for each packet of 100 detected spikes processed by the NSS\n",
    "# peaks = dataset[\"raster\"]\n",
    "# gtr = dataset[\"gt_raster\"]\n",
    "# spike_packet = 300\n",
    "# fscore_nss_packet = []\n",
    "# for i in range(0, len(peaks) - spike_packet, spike_packet):\n",
    "#     mask_pred = (peaks >= peaks[i]) & (peaks < peaks[i + spike_packet])\n",
    "#     mask_gtr = (gtr[0] >= peaks[i]) & (gtr[0] < peaks[i + spike_packet])\n",
    "#     gtsort_comp = GTSortingComparison(\n",
    "#         label_nss[mask_pred],\n",
    "#         peaks[mask_pred],\n",
    "#         gtr[:, mask_gtr],\n",
    "#         fs,\n",
    "#         delta_time=2,\n",
    "#     )\n",
    "#     fscore_nss_packet.append(gtsort_comp.get_fscore().round(4))\n",
    "# fscore_nss_packet = np.array(fscore_nss_packet)\n",
    "# print(f\"F1s NSS packet: {fscore_nss_packet.mean()*100:.2f}%\")\n",
    "\n",
    "# # plot the fscore for each packet of 100 detected spikes processed by the NSS\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.plot(fscore_nss_packet)\n",
    "# # add snr as legend for each line\n",
    "# ax.legend(dataset[\"snr\"].round(2))\n",
    "# ax.set_xlabel(\"Number of spikes processed\")\n",
    "# ax.set_ylabel(\"F1-score\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute stats - Simulated Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "nneurons = 5\n",
    "seed = 0\n",
    "# compute fscore every packet of Ns processed spikes\n",
    "spike_packet = 100\n",
    "\n",
    "# Simulated datasets\n",
    "datasets = [209]\n",
    "\n",
    "# # HC-1 dataset\n",
    "# datasets = [\n",
    "#     \"data/hc1/hc1_d533101_dth5_tmax240_noburst.h5\",\n",
    "#     \"data/hc1/hc1_d561104_dth5_tmax200.h5\",\n",
    "#     \"data/hc1/hc1_d561105_dth4_tmax240.h5\",\n",
    "#     \"data/hc1/hc1_d561106_dth4_tmax240.h5\",\n",
    "# ]\n",
    "# tmax_values = [240, 200, 240, 240]\n",
    "# ds_labels = [\"d533101\", \"d561104\", \"d561105\", \"d561106\"]\n",
    "\n",
    "N_values = [1, 2, 4, 8, 16, 32]\n",
    "nmodel = \"TDQ\"\n",
    "\n",
    "for d, ds in enumerate(datasets):\n",
    "    print(f\"--- Running NSS on dataset {ds} ---\")\n",
    "    data = f\"data/tetrode/tetrode{ds}_n5_static.h5\"\n",
    "    dataset, dataloader = load_dataset(data, tmax=240)\n",
    "    # dataset, dataloader = load_dataset(ds, tmax=tmax_values[d])\n",
    "    peaks = dataset[\"raster\"]\n",
    "    gtr = dataset[\"gt_raster\"]\n",
    "\n",
    "    for k, N in enumerate(N_values):\n",
    "        print(f\"*** N={N} ***\")\n",
    "        labels, nsp, _ = run_nss(\n",
    "            dataloader, seed, N, model=nmodel, ths=[0.04, 0.04], n_atoms=[120, 10]\n",
    "        )\n",
    "        print(f\"median nsp = {np.mean(nsp):.1f} +/- {np.std(nsp):.1f}\")\n",
    "\n",
    "        # compute fscore every packet of Ns spikes processed\n",
    "        spike_processed, fscore_nss = [], []\n",
    "        for i in range(0, len(peaks), spike_packet):\n",
    "            if i + spike_packet >= len(peaks):\n",
    "                break\n",
    "            mask_pred = (peaks >= peaks[i]) & (peaks < peaks[i + spike_packet])\n",
    "            mask_gtr = (gtr[0] >= peaks[i]) & (gtr[0] < peaks[i + spike_packet])\n",
    "            gtsort_comp = GTSortingComparison(\n",
    "                labels[mask_pred],\n",
    "                peaks[mask_pred],\n",
    "                gtr[:, mask_gtr],\n",
    "                fs,\n",
    "                delta_time=2,\n",
    "            )\n",
    "            score = gtsort_comp.get_fscore()\n",
    "            if not score.size > 0:\n",
    "                continue\n",
    "            else:\n",
    "                spike_processed.append(i + spike_packet)\n",
    "                fscore_nss.append(score)\n",
    "        if nneurons > 1:\n",
    "            fscore_nss = np.array(fscore_nss).T\n",
    "        else:\n",
    "            fscore_nss = np.array(fscore_nss).reshape(nneurons, -1)\n",
    "        spike_processed = np.array(spike_processed)\n",
    "        print(f\"F1s NSS packet: {fscore_nss.mean()*100:.2f}%\")\n",
    "\n",
    "        # plot\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(fscore_nss.T)\n",
    "        ax.set_xlabel(f\"Number of processed packet ({spike_packet} spikes)\")\n",
    "        ax.set_ylabel(\"F1-score\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Evolution of F-score over time for simulated tetrode dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load results for simulated tetrode only\n",
    "res_df = pd.read_pickle(\"logs/figure4_variation_N/tetrode_n5_variation_N_nss.pkl\")\n",
    "res209 = pd.read_pickle(\"logs/figure4_variation_N/tetrode209_n5_variation_N_nss.pkl\")\n",
    "res_tr = pd.read_pickle(\"logs/figure4_variation_N/hc1_variation_N_nss.pkl\")\n",
    "res_df = res_df[~(res_df[\"dataset_name\"] == \"tetrode209\")]\n",
    "res_all = pd.concat([res_df, res209, res_tr])\n",
    "res_all = res_all[res_all[\"N\"] == 2]\n",
    "\n",
    "res_all[\"dataset_name\"] = res_all[\"dataset_name\"].replace(\n",
    "    {\n",
    "        \"tetrode209\": \"TS1\",\n",
    "        \"tetrode49\": \"TS2\",\n",
    "        \"tetrode62\": \"TS3\",\n",
    "        \"tetrode71\": \"TS4\",\n",
    "        \"d533101\": \"TR1\",\n",
    "        \"d561104\": \"TR2\",\n",
    "        \"d561105\": \"TR3\",\n",
    "        \"d561106\": \"TR4\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# compute mean f-score over snr for each trial\n",
    "res_df2_grp = (\n",
    "    res_all.groupby([\"dataset_name\", \"trial\", \"spike_processed\"])\n",
    "    .agg({\"fscore\": \"mean\"})\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute mean of fscore between 0 and 200 spikes then between 200 and 400 spikes and so on\n",
    "spike_packet = 4\n",
    "fscore_nss = pd.DataFrame(\n",
    "    columns=[\"dataset_name\", \"trial\", \"spike_processed\", \"fscore_avg\"]\n",
    ")\n",
    "for ds in res_df2_grp[\"dataset_name\"].unique():\n",
    "    res_df2_grp_ds = res_df2_grp[res_df2_grp[\"dataset_name\"] == ds]\n",
    "    for t in res_df2_grp_ds[\"trial\"].unique():\n",
    "        subset = res_df2_grp_ds[res_df2_grp_ds[\"trial\"] == t]\n",
    "        subset = subset.sort_values(by=\"spike_processed\")\n",
    "        for i in range(0, len(subset), spike_packet):\n",
    "            if i + spike_packet >= len(subset):\n",
    "                break\n",
    "            fscore_avg = np.mean(subset[\"fscore\"].iloc[i : i + spike_packet])\n",
    "            fscore_nss = pd.concat(\n",
    "                [\n",
    "                    fscore_nss,\n",
    "                    pd.DataFrame(\n",
    "                        {\n",
    "                            \"dataset_name\": [ds],\n",
    "                            \"trial\": [t],\n",
    "                            \"spike_processed\": [subset[\"spike_processed\"].iloc[i]],\n",
    "                            \"fscore_avg\": [fscore_avg],\n",
    "                        }\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "\n",
    "# multiply by 100 to get percentage\n",
    "fscore_nss[\"fscore_avg\"] *= 100\n",
    "fscore_nss_grp = fscore_nss.groupby([\"dataset_name\", \"spike_processed\"]).agg(\n",
    "    {\"fscore_avg\": [\"mean\", \"std\"]}\n",
    ")\n",
    "\n",
    "fscore_nss_grp.columns = [\"fscore_mean\", \"fscore_std\"]\n",
    "fscore_nss_grp = fscore_nss_grp.reset_index()\n",
    "# compute confidence interval 95% for each dataset\n",
    "z = 1.96\n",
    "fscore_nss_grp[\"fscore_ci\"] = (\n",
    "    z * fscore_nss_grp[\"fscore_std\"] / np.sqrt(res_df2_grp[\"trial\"].nunique())\n",
    ")\n",
    "\n",
    "# plot\n",
    "dst_labels = [\n",
    "    [\"$TS_{1}$\", \"$TS_{2}$\", \"$TS_{3}$\", \"$TS_{4}$\"],\n",
    "    [\"$TR_{1}$\", \"$TR_{2}$\", \"$TR_{3}$\", \"$TR_{4}$\"],\n",
    "]\n",
    "plt.style.use(\"seaborn-v0_8-paper\")\n",
    "fig, axs = plt.subplots(1, 2, figsize=(6.5, 2.5), dpi=150)\n",
    "axs = axs.ravel()\n",
    "for ax in axs:\n",
    "    ax.tick_params(axis=\"both\", which=\"major\", labelsize=10)\n",
    "    ax.tick_params(axis=\"both\", which=\"minor\", labelsize=8)\n",
    "\n",
    "for k in range(2):\n",
    "    dst = \"TS\" if k == 0 else \"TR\"\n",
    "    line_labels = dst_labels[k]\n",
    "    cmap = sns.color_palette(\"colorblind\", 8)[k * 4 : (k + 1) * 4]\n",
    "    ax = axs[k]\n",
    "    fscore = fscore_nss_grp[fscore_nss_grp[\"dataset_name\"].str.contains(dst)]\n",
    "    for d, ds in enumerate(fscore[\"dataset_name\"].unique()):\n",
    "        subset = fscore_nss_grp[fscore_nss_grp[\"dataset_name\"] == ds]\n",
    "        ax.plot(\n",
    "            subset[\"spike_processed\"],\n",
    "            subset[\"fscore_mean\"],\n",
    "            label=line_labels[d],\n",
    "            c=cmap[d],\n",
    "        )\n",
    "        # plot asymptote with the text \"$F_{1\\infty}$\"\n",
    "        sp_max = 10001 if k == 0 else 4001\n",
    "        asymp = subset[\n",
    "            subset[\"spike_processed\"] >= subset[\"spike_processed\"].max() // 2\n",
    "        ][\"fscore_mean\"].mean()\n",
    "        len_asymp = 10001 if k == 0 else 2800\n",
    "        ax.plot([0, len_asymp], [asymp, asymp], c=cmap[d], ls=\"--\")\n",
    "        ax.fill_between(\n",
    "            subset[\"spike_processed\"],\n",
    "            subset[\"fscore_mean\"] - subset[\"fscore_ci\"],\n",
    "            subset[\"fscore_mean\"] + subset[\"fscore_ci\"],\n",
    "            alpha=0.3,\n",
    "            color=cmap[d],\n",
    "        )\n",
    "    ax.spines[[\"top\", \"right\"]].set_visible(False)\n",
    "    ax.set_ylabel(\"$F_{1}$-score (%)\", fontsize=12)\n",
    "    ax.set_xlabel(\"Number of processed spikes ($x10^{3}$)\", fontsize=12)\n",
    "\n",
    "    ax.set_xlim(-100, sp_max)\n",
    "    ax.set_yticks(np.arange(0, 110, 20))\n",
    "    ax.set_yticks(np.arange(0, 110, 10), minor=True)\n",
    "    ax.set_xticks(np.arange(0, sp_max, 2000))\n",
    "    ax.set_xticklabels(np.arange(0, int(sp_max * 1e-3 + 1), 2).round(1), fontsize=11)\n",
    "    ax.set_xticks(np.arange(0, sp_max, 1000), minor=True)\n",
    "\n",
    "    lines = ax.get_lines()\n",
    "    lgd1 = ax.legend(\n",
    "        lines[::2],\n",
    "        line_labels,\n",
    "        fontsize=11,\n",
    "        # bbox_to_anchor=(0.3, 0.4),\n",
    "        loc=\"best\" if k == 0 else \"lower right\",\n",
    "        ncol=2 if k == 0 else 1,\n",
    "        handlelength=0.7,\n",
    "        edgecolor=\"w\",\n",
    "    )\n",
    "    asymp_lgd = [\n",
    "        Line2D([0], [0], color=\"black\", ls=\"--\", label=\"NSS-1bit $F_{1\\infty}$\", lw=1)\n",
    "    ]\n",
    "    lgd2 = ax.legend(\n",
    "        handles=asymp_lgd,\n",
    "        bbox_to_anchor=(0.35, 0.85),\n",
    "        handlelength=0.8,\n",
    "        fontsize=11,\n",
    "        edgecolor=\"w\",\n",
    "    )\n",
    "    ax.add_artist(lgd1)\n",
    "    ax.add_artist(lgd2)\n",
    "# ax.legend(fontsize=11, ncols=4, loc=\"upper center\", mode=\"expand\", handlelength=0.8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(\n",
    "    \"figures/fig4a_tetrode_fscore_evolution.svg\",\n",
    "    format=\"svg\",\n",
    "    dpi=150,\n",
    "    bbox_inches=\"tight\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Evolution of F1-score for HC-1 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read pickle files for real tetrode dataset (HC-1)\n",
    "res_pcakm = pd.read_pickle(f\"logs/figure5_benchmark/hc1_pcakm.pkl\")\n",
    "res_wc = pd.read_pickle(f\"logs/figure5_benchmark/hc1_wc.pkl\")\n",
    "res_nss = pd.read_pickle(f\"logs/figure5_benchmark/hc1_d533101_N2-4_nss.pkl\")\n",
    "res_nss = res_nss[res_nss[\"N\"] == 4]\n",
    "\n",
    "\n",
    "raster = load_dataset(\"data/hc1/hc1_d533101_dth5_tmax240_noburst.h5\", tmax=240)[0][\n",
    "    \"raster\"\n",
    "]\n",
    "spikes_60s = np.sum(raster < 60 * fs)\n",
    "\n",
    "# plot\n",
    "ds = \"d533101\"\n",
    "subset_nss = res_nss[\n",
    "    (res_nss[\"dataset_name\"] == ds) & (res_nss[\"spike_processed\"] >= spikes_60s)\n",
    "]\n",
    "subset_pcakm = res_pcakm[\n",
    "    (res_pcakm[\"dataset_name\"] == ds) & (res_pcakm[\"spike_processed\"] >= spikes_60s)\n",
    "]\n",
    "subset_wc = res_wc[\n",
    "    (res_wc[\"dataset_name\"] == ds) & (res_wc[\"spike_processed\"] >= spikes_60s)\n",
    "]\n",
    "time = np.array([raster[p] / fs for p in subset_nss[\"spike_processed\"].unique()])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(3, 2.5), dpi=150)\n",
    "ax.tick_params(axis=\"both\", which=\"major\", labelsize=10)\n",
    "ax.tick_params(axis=\"both\", which=\"minor\", labelsize=8)\n",
    "z = 1.96\n",
    "\n",
    "# PC-KM\n",
    "mean_pcakm = (\n",
    "    subset_pcakm.groupby(\"spike_processed\")\n",
    "    .agg({\"fscore\": [\"mean\", \"std\"]})\n",
    "    .reset_index()\n",
    ")\n",
    "mean_pcakm.columns = [\"spike_processed\", \"mean\", \"std\"]\n",
    "mean_pcakm[\"ci\"] = z * mean_pcakm[\"std\"] / np.sqrt(subset_pcakm[\"trial\"].nunique())\n",
    "ax.plot(time, mean_pcakm[\"mean\"], label=\"PCA+KMeans\", c=\"r\")\n",
    "ax.fill_between(\n",
    "    time,\n",
    "    mean_pcakm[\"mean\"] - mean_pcakm[\"ci\"],\n",
    "    mean_pcakm[\"mean\"] + mean_pcakm[\"ci\"],\n",
    "    color=\"r\",\n",
    "    alpha=0.2,\n",
    ")\n",
    "\n",
    "# WC\n",
    "mean_wc = (\n",
    "    subset_wc.groupby(\"spike_processed\").agg({\"fscore\": [\"mean\", \"std\"]}).reset_index()\n",
    ")\n",
    "mean_wc.columns = [\"spike_processed\", \"mean\", \"std\"]\n",
    "mean_wc[\"ci\"] = z * mean_wc[\"std\"] / np.sqrt(subset_wc[\"trial\"].nunique())\n",
    "ax.plot(time, mean_wc[\"mean\"], label=\"WaveClus3\", c=\"b\")\n",
    "# ax.fill_between(\n",
    "#     time,\n",
    "#     mean_wc[\"mean\"] - mean_wc[\"ci\"],\n",
    "#     mean_wc[\"mean\"] + mean_wc[\"ci\"],\n",
    "#     color=\"b\",\n",
    "#     alpha=0.2,\n",
    "# )\n",
    "\n",
    "# NSS\n",
    "mean_nss = (\n",
    "    subset_nss.groupby(\"spike_processed\").agg({\"fscore\": [\"mean\", \"std\"]}).reset_index()\n",
    ")\n",
    "mean_nss.columns = [\"spike_processed\", \"mean\", \"std\"]\n",
    "mean_nss[\"ci\"] = z * mean_nss[\"std\"] / np.sqrt(subset_nss[\"trial\"].nunique())\n",
    "ax.plot(time, mean_nss[\"mean\"], label=\"NSS\", c=\"g\")\n",
    "ax.fill_between(\n",
    "    time,\n",
    "    mean_nss[\"mean\"] - mean_nss[\"ci\"],\n",
    "    mean_nss[\"mean\"] + mean_nss[\"ci\"],\n",
    "    color=\"g\",\n",
    "    alpha=0.2,\n",
    ")\n",
    "ax.set_ylim(0.1, 1.01)\n",
    "ax.set_yticks(np.arange(0.1, 1.1, 0.1), minor=True)\n",
    "ax.set_yticks(np.arange(0.1, 1.1, 0.2))\n",
    "ax.set_yticklabels(np.arange(0.1, 1.1, 0.2).round(2), fontsize=11)\n",
    "ax.set_xticks(np.arange(60, 241, 30), minor=True)\n",
    "ax.set_xticks(np.arange(60, 241, 60))\n",
    "ax.set_xticklabels(np.arange(60, 241, 60), fontsize=11)\n",
    "\n",
    "\n",
    "ax.set_xlabel(\"Time (s)\", fontsize=12)\n",
    "ax.set_ylabel(\"$F_{1}$-score\", fontsize=12)\n",
    "ax.legend(handlelength=1, fontsize=9, edgecolor=\"w\", loc=\"lower right\")\n",
    "ax.spines[[\"top\", \"right\"]].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\n",
    "#     \"figures/fig6c_hc1d533101_fscore_evolution.svg\",\n",
    "#     format=\"svg\",\n",
    "#     dpi=150,\n",
    "#     bbox_inches=\"tight\",\n",
    "# )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_nss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lca_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
