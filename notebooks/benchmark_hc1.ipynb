{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import h5py as h5\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from model.Lca import init_lca\n",
    "from model.NSS import NSS\n",
    "from utils.build_dataset import init_dataset, init_dataset_online, init_dataloader\n",
    "from tqdm import tqdm\n",
    "import MEAutility as mu\n",
    "from utils.metrics import GTSortingComparison\n",
    "\n",
    "\n",
    "def compute_fscore_evolution(\n",
    "    label, detected_raster, gtr, time_step=10, trange=(0, 201), fs=10000\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute the fscore evolution over time for each neuron\n",
    "    \"\"\"\n",
    "\n",
    "    n_neurons = len(np.unique(gtr[1]))\n",
    "    sorting_perf = GTSortingComparison(\n",
    "        label, detected_raster, gtr, fs, delta_time=2\n",
    "    ).get_sorting_perf(match_mode=\"hungarian\")\n",
    "    best_match_12 = sorting_perf.best_match_12.to_numpy().astype(int)\n",
    "    # Get either 'TP', or 'FP' of 'FN' for each detected spikes and gt spikes\n",
    "    labels_comp_wvs = np.zeros((n_neurons, detected_raster.size), dtype=object)\n",
    "    id_spike = np.zeros(\n",
    "        label.size, dtype=int\n",
    "    )  # index to be incremented of the spike for each atom\n",
    "    for i in range(detected_raster.size):\n",
    "        if label[i] in best_match_12:\n",
    "            associated_unit = np.argwhere(best_match_12 == label[i]).flatten()[0]\n",
    "            labels_comp_wvs[associated_unit][i] = sorting_perf.get_labels2(\n",
    "                unit_id=label[i]\n",
    "            )[0][id_spike[label[i]]]\n",
    "            id_spike[label[i]] += 1\n",
    "        else:\n",
    "            pass\n",
    "    labels_comp_wvs = labels_comp_wvs.squeeze()\n",
    "\n",
    "    labels_comp_gt = np.zeros(gtr.shape[1], dtype=object)\n",
    "    id_unit = np.zeros(\n",
    "        1, dtype=int\n",
    "    )  # index to be incremented of the spike for each unit\n",
    "    for i in range(gtr.shape[1]):\n",
    "        labels_comp_gt[i] = sorting_perf.get_labels1(unit_id=gtr[1, i])[0][\n",
    "            id_unit[gtr[1, i]]\n",
    "        ]\n",
    "        id_unit[gtr[1, i]] += 1\n",
    "\n",
    "    t_range = np.arange(trange[0], trange[1] + 1, time_step) * fs\n",
    "\n",
    "    # metrics\n",
    "    tps = np.zeros(t_range.size - 1)\n",
    "    fps = np.zeros_like(tps)\n",
    "    fns = np.zeros_like(tps)\n",
    "    tps_gt = np.zeros_like(tps)\n",
    "    fscore = np.zeros((n_neurons, t_range.size - 1))\n",
    "\n",
    "    for i in range(1, t_range.size):\n",
    "        # detected spikes\n",
    "        mask_wvs = (detected_raster >= t_range[i - 1]) & (detected_raster < t_range[i])\n",
    "        tps[i - 1] = np.sum(labels_comp_wvs[mask_wvs] == \"TP\")\n",
    "        fps[i - 1] = np.sum(labels_comp_wvs[mask_wvs] == \"FP\")\n",
    "\n",
    "        for j in range(n_neurons):  # gt spikes\n",
    "            mask_gt = (gtr[0] >= t_range[i - 1]) & (gtr[0] < t_range[i]) & (gtr[1] == j)\n",
    "            fns[i - 1] += np.sum(labels_comp_gt[mask_gt] == \"FN\")\n",
    "            fscore[j, i - 1] = (\n",
    "                2 * tps[i - 1] / (2 * tps[i - 1] + fps[i - 1] + fns[i - 1])\n",
    "            )\n",
    "\n",
    "    return fscore.squeeze(), t_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mea_probe = mu.return_mea(\"tetrode\")\n",
    "fs = 10000\n",
    "batch_size = 16\n",
    "\n",
    "with h5.File(\"data/hc1_d533101_dth6_noburst.h5\", \"r\") as f:\n",
    "    wvs = np.array(f[\"wvs\"][:], dtype=np.float32)\n",
    "    gt_raster = np.array(f[\"gt_raster\"][:], dtype=np.int32)\n",
    "    snr = np.array(f[\"snr\"], dtype=np.float32)\n",
    "    peaks_idx = np.array(f[\"peaks_idx\"][:], dtype=np.int32)\n",
    "    # wvs_gt = np.array(f[\"wvs_gt\"][:], dtype=np.float32)\n",
    "f.close()\n",
    "n_neurons = np.unique(gt_raster[1]).shape[0]\n",
    "\n",
    "# # normalize waveforms with l2-norm\n",
    "# l2_norm = np.linalg.norm(wvs, ord=2, axis=1)\n",
    "# if np.sum(l2_norm < 1e-6) > 0:\n",
    "#     print(\"Warning: some waveforms are null\")\n",
    "# wvs = wvs / np.linalg.norm(wvs, ord=2, axis=1)[:, None]\n",
    "wvs = wvs / np.max(np.abs(wvs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "burst_time = 0  # 110s\n",
    "mask_detected = np.where(peaks_idx > burst_time * fs)[0]\n",
    "mask_gt = np.where(gt_raster[0] > burst_time * fs)[0]\n",
    "wvs2 = wvs[mask_detected, :]\n",
    "peaks_idx2 = peaks_idx[mask_detected]\n",
    "gt_raster2 = gt_raster[:, mask_gt]\n",
    "dataloader2 = init_dataloader(wvs2, peaks_idx2, batch_size=batch_size, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best hp for simulated tetrode dataset: fscore=98% : ##### with normalization ######\n",
    "# 'natoms1': 108, 'tau': 0.002, 'threshold1': 0.03, 'threshold2': 0.076, 'lr': 0.079}\n",
    "# coef_max = 0.8\n",
    "\n",
    "# best hp for simulated tetrode dataset: fscore=93.5% : ##### with scaling with absolute max ######\n",
    "# 'natoms1': 462, 'tau': 0.0028, 'threshold1': 0.118, 'threshold2': 0.015, 'lr': 0.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set hyper-parameters and init NSS layers\n",
    "ntrials = 1\n",
    "time_step = 10\n",
    "trange = (20, 241)\n",
    "fscore = np.zeros((ntrials, int((trange[1] - trange[0]) / time_step)))\n",
    "for t in range(ntrials):\n",
    "    print(f\"t={t}\")\n",
    "    seed = t\n",
    "    params_nss = {\n",
    "        \"layer1\": {\n",
    "            \"n_atoms\": 360,\n",
    "            \"tau\": 1e-3,\n",
    "            \"threshold\": 0.04,  # 0.05\n",
    "            \"iters\": 100,\n",
    "            \"lr\": 0.05,\n",
    "            \"n_model\": \"TDQ\",\n",
    "            \"q\": 2**8 - 1,\n",
    "            \"seed\": seed,\n",
    "        },\n",
    "        \"layer2\": {\n",
    "            \"n_atoms\": 20,\n",
    "            \"tau\": 1e-3,\n",
    "            \"threshold\": 0.08,\n",
    "            \"iters\": 100,\n",
    "            \"lr\": 0.05,\n",
    "            \"n_model\": \"TDQ\",\n",
    "            \"q\": 2**8 - 1,\n",
    "            \"seed\": seed,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    ## init lca1\n",
    "    lca1 = init_lca(\n",
    "        fs=fs,\n",
    "        input_size=wvs.shape[1],  # dataset[\"test\"][\"wv\"].shape[1],\n",
    "        natoms=params_nss[\"layer1\"][\"n_atoms\"],\n",
    "        tau=params_nss[\"layer1\"][\"tau\"],\n",
    "        threshold=params_nss[\"layer1\"][\"threshold\"],\n",
    "        iters=params_nss[\"layer1\"][\"iters\"],\n",
    "        q=params_nss[\"layer1\"][\"q\"],\n",
    "        beta=0,\n",
    "        lr=params_nss[\"layer1\"][\"lr\"],\n",
    "        n_model=params_nss[\"layer1\"][\"n_model\"],\n",
    "        seed=params_nss[\"layer1\"][\"seed\"],\n",
    "    )\n",
    "\n",
    "    lca2 = init_lca(\n",
    "        fs=fs,\n",
    "        input_size=params_nss[\"layer1\"][\"n_atoms\"],\n",
    "        natoms=params_nss[\"layer2\"][\"n_atoms\"],\n",
    "        tau=params_nss[\"layer2\"][\"tau\"],\n",
    "        threshold=params_nss[\"layer2\"][\"threshold\"],\n",
    "        iters=params_nss[\"layer2\"][\"iters\"],\n",
    "        q=params_nss[\"layer2\"][\"q\"],\n",
    "        beta=0,\n",
    "        lr=params_nss[\"layer2\"][\"lr\"],\n",
    "        n_model=params_nss[\"layer2\"][\"n_model\"],\n",
    "        seed=params_nss[\"layer2\"][\"seed\"],\n",
    "        plus_one=True,\n",
    "    )\n",
    "    lca2.positive_D = True\n",
    "\n",
    "    ## Run NSS not sequential train\n",
    "    input_dataloader = dataloader2  # dataloaders[\"train\"] #\n",
    "    detected_raster = peaks_idx2  # dataset[\"train\"][\"raster\"] #   #\n",
    "    gtr = gt_raster2  # dataset[\"train\"][\"gt_raster\"] #   #\n",
    "    n_batch = len(input_dataloader)\n",
    "\n",
    "    nss = NSS(lca1, lca2, batch_size, decay_lr=True)\n",
    "    # nss.lca1.mode, nss.lca2.mode = \"train\", \"train\"\n",
    "    # nss.lca1.mode, nss.lca2.mode = \"eval\", \"eval\"\n",
    "    lca1_a, lca2_a, lasso1_b, lasso2_b = nss(input_dataloader)  # run nss\n",
    "    label = nss.label\n",
    "\n",
    "    fscore[t, :], t_range = compute_fscore_evolution(\n",
    "        label, detected_raster, gtr, time_step, trange, fs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot fscore evolution mean and std\n",
    "# fscore : (ntrials, t_range)\n",
    "trange = (0, 241)\n",
    "fscore = np.zeros((ntrials, int((trange[1] - trange[0]) / time_step)))\n",
    "fscore[t, :], t_range = compute_fscore_evolution(\n",
    "    label, detected_raster, gtr, time_step, trange, fs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6), dpi=100, tight_layout=True)\n",
    "ax.set_title(\"F-score evolution\")\n",
    "ax.plot(t_range[:-1] / fs, np.mean(fscore, axis=0), color=\"k\", lw=2)\n",
    "ax.fill_between(\n",
    "    t_range[:-1] / fs,\n",
    "    np.mean(fscore, axis=0) - np.std(fscore, axis=0),\n",
    "    np.mean(fscore, axis=0) + np.std(fscore, axis=0),\n",
    "    alpha=0.2,\n",
    ")\n",
    "ax.set_xlabel(\"Time (s)\")\n",
    "ax.set_ylabel(\"F-score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute accuracy and F1-score\n",
    "detected_raster = peaks_idx2  # dataset[\"train\"][\"raster\"] #   #\n",
    "gtr = gt_raster2  # dataset[\"train\"][\"gt_raster\"] #   #\n",
    "gtsort_comp = GTSortingComparison(label, detected_raster, gtr, fs, delta_time=1)\n",
    "acc = gtsort_comp.get_accuracy()\n",
    "fscore = gtsort_comp.get_fscore()\n",
    "print(f\"Accuracy: {acc.mean()*100:.3f}\")\n",
    "print(f\"F1-score: {fscore.mean()*100:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot label histogram\n",
    "plt.figure()\n",
    "plt.hist(label, bins=10)\n",
    "plt.title(\"LCA2 label histogram\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Eval for loihi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "dataset, dataloaders = init_dataset_online(\n",
    "    wvs,\n",
    "    peaks_idx,\n",
    "    gt_raster,\n",
    "    eval_size=0.4,\n",
    "    test_size=0.2,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_nss = {\n",
    "    \"layer1\": {\n",
    "        \"n_atoms\": 360,\n",
    "        \"tau\": 1e-3,\n",
    "        \"threshold\": 0.04,  # 0.05\n",
    "        \"iters\": 100,\n",
    "        \"lr\": 0.05,\n",
    "        \"n_model\": \"TDQ\",\n",
    "        \"q\": 2**1 - 1,\n",
    "        \"seed\": seed,\n",
    "    },\n",
    "    \"layer2\": {\n",
    "        \"n_atoms\": 20,\n",
    "        \"tau\": 1e-3,\n",
    "        \"threshold\": 0.08,\n",
    "        \"iters\": 100,\n",
    "        \"lr\": 0.05,\n",
    "        \"n_model\": \"TDQ\",\n",
    "        \"q\": 2**1 - 1,\n",
    "        \"seed\": seed,\n",
    "    },\n",
    "}\n",
    "\n",
    "## init lca1\n",
    "lca1 = init_lca(\n",
    "    fs=fs,\n",
    "    input_size=wvs.shape[1],  # dataset[\"test\"][\"wv\"].shape[1],\n",
    "    natoms=params_nss[\"layer1\"][\"n_atoms\"],\n",
    "    tau=params_nss[\"layer1\"][\"tau\"],\n",
    "    threshold=params_nss[\"layer1\"][\"threshold\"],\n",
    "    iters=params_nss[\"layer1\"][\"iters\"],\n",
    "    q=params_nss[\"layer1\"][\"q\"],\n",
    "    beta=0,\n",
    "    lr=params_nss[\"layer1\"][\"lr\"],\n",
    "    n_model=params_nss[\"layer1\"][\"n_model\"],\n",
    "    seed=params_nss[\"layer1\"][\"seed\"],\n",
    ")\n",
    "\n",
    "lca2 = init_lca(\n",
    "    fs=fs,\n",
    "    input_size=params_nss[\"layer1\"][\"n_atoms\"],\n",
    "    natoms=params_nss[\"layer2\"][\"n_atoms\"],\n",
    "    tau=params_nss[\"layer2\"][\"tau\"],\n",
    "    threshold=params_nss[\"layer2\"][\"threshold\"],\n",
    "    iters=params_nss[\"layer2\"][\"iters\"],\n",
    "    q=params_nss[\"layer2\"][\"q\"],\n",
    "    beta=0,\n",
    "    lr=params_nss[\"layer2\"][\"lr\"],\n",
    "    n_model=params_nss[\"layer2\"][\"n_model\"],\n",
    "    seed=params_nss[\"layer2\"][\"seed\"],\n",
    "    plus_one=True,\n",
    ")\n",
    "lca2.positive_D = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train\n",
    "nss = NSS(lca1, lca2, batch_size)\n",
    "lca1_a, lca2_a, lasso1_b, lasso2_b = nss(dataloaders[\"train\"])  # run nss\n",
    "## eval\n",
    "nss.lca1.mode, nss.lca2.mode = \"eval\", \"eval\"\n",
    "lca1_a, lca2_a, lasso1_b, lasso2_b = nss(dataloaders[\"eval\"])  # run nss\n",
    "label = nss.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute accuracy and F1-score\n",
    "detected_raster = dataset[\"eval\"][\"raster\"]  #   #\n",
    "gtr = dataset[\"eval\"][\"gt_raster\"]  #   #\n",
    "gtsort_comp = GTSortingComparison(label, detected_raster, gtr, fs, delta_time=2)\n",
    "acc = gtsort_comp.get_accuracy()\n",
    "fscore = gtsort_comp.get_fscore()\n",
    "print(f\"Accuracy: {acc.mean()*100:.3f}\")\n",
    "print(f\"F1-score: {fscore.mean()*100:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save NSS model and dataset\n",
    "import pickle\n",
    "\n",
    "with h5.File(\"logs/saved_dict_coeffs_h5/hc1_d533101/trained_nss.h5\", \"w\") as f:\n",
    "    f.create_dataset(\"D1\", data=nss.lca1.D.cpu().numpy(), dtype=np.float32)\n",
    "    f.create_dataset(\"D2\", data=nss.lca2.D.cpu().numpy(), dtype=np.float32)\n",
    "\n",
    "    f.create_dataset(\"wvs\", data=dataset[\"eval\"][\"wv\"], dtype=np.float32)\n",
    "    f.create_dataset(\"gt_raster\", data=dataset[\"eval\"][\"gt_raster\"], dtype=np.int32)\n",
    "    f.create_dataset(\"peaks_idx\", data=dataset[\"eval\"][\"raster\"], dtype=np.int32)\n",
    "    f.create_dataset(\"label\", data=label)\n",
    "    # f.create_dataset(\"lasso_test\", data=lasso_test)\n",
    "f.close()\n",
    "\n",
    "with open(\"logs/saved_dict_coeffs_h5/hc1_d533101/params_nss.pkl\", \"wb\") as f:\n",
    "    pickle.dump(params_nss, f)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lca_spikesorting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
