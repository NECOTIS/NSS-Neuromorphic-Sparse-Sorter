{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import spikeinterface.full as si\n",
    "from sparsesorter.models.nss import NSS\n",
    "from sparsesorter.utils.metrics import compute_fscore_evolution, SortingMetrics\n",
    "from sparsesorter.utils.dataloader import (\n",
    "    build_dataloader,\n",
    "    init_dataloader,\n",
    "    compute_detection_performance,\n",
    ")\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "data_path = Path(\"../data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False Positive Impact on NSS spike sorting F1-score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"TS1\", \"TS2\", \"TS3\", \"TS4\"]\n",
    "with_fps = [False, True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "res = {\n",
    "    \"TS1\": {\"w/ FP\": [], \"w/o FP\": []},\n",
    "    \"TS2\": {\"w/ FP\": [], \"w/o FP\": []},\n",
    "    \"TS3\": {\"w/ FP\": [], \"w/o FP\": []},\n",
    "    \"TS4\": {\"w/ FP\": [], \"w/o FP\": []},\n",
    "}\n",
    "for ds in datasets:\n",
    "    print(f\"### dataset {ds} ###\")\n",
    "    for fps in with_fps:\n",
    "        print(f\"## {'with' if fps else 'without'} FP ##\")\n",
    "        for s in range(1):\n",
    "            print(f\"# seed {s} #\")\n",
    "            ds_file = data_path / f\"{ds}.h5\"\n",
    "            dataset, dataloader = build_dataloader(ds_file, batch_size=batch_size)\n",
    "            if not fps:\n",
    "                fs = dataset[\"fs\"]\n",
    "                gtr = dataset[\"gt_raster\"]\n",
    "                snr = dataset[\"snr\"]\n",
    "                peaks_idx = dataset[\"raster\"]\n",
    "                peaks_idx_copy = np.copy(peaks_idx)\n",
    "                unique, counts = np.unique(gtr[1], return_counts=True)\n",
    "                delta_time = 1\n",
    "                tp = np.zeros(len(snr))\n",
    "                fn = np.zeros_like(tp)\n",
    "                fp = 0\n",
    "                not_detected_gt_spikes = []\n",
    "                well_detected_spikes = []\n",
    "                labels_peaks = -1 * np.ones(len(peaks_idx))\n",
    "                for i in range(gtr.shape[1]):\n",
    "                    idx = np.where(\n",
    "                        np.abs(peaks_idx_copy - gtr[0, i]) <= delta_time * fs / 1000\n",
    "                    )  # search for a spike in a 1ms range\n",
    "                    if idx[0].size > 0:\n",
    "                        tp[gtr[1, i]] += 1\n",
    "                        well_detected_spikes.append(i)\n",
    "                        idx_closest = np.argmin(np.abs(peaks_idx_copy - gtr[0, i]))\n",
    "                        # store the label gtr[1, i] in the labels_peaks array at the index of the detected peak in peaks_idx\n",
    "                        labels_peaks[\n",
    "                            np.where(peaks_idx == peaks_idx_copy[idx_closest])\n",
    "                        ] = gtr[1, i]\n",
    "                        peaks_idx_copy = np.delete(peaks_idx_copy, idx_closest)\n",
    "\n",
    "                    else:\n",
    "                        fn[gtr[1, i]] += 1\n",
    "                        not_detected_gt_spikes.append(i)\n",
    "                fp = len(peaks_idx) - len(well_detected_spikes)\n",
    "                precision = tp / counts\n",
    "                recall = tp / (tp + fn)\n",
    "                fprate = fp / (fp + tp)\n",
    "\n",
    "                mask = labels_peaks != -1\n",
    "                dataset[\"wvs\"] = dataset[\"wvs\"][mask]\n",
    "                dataset[\"raster\"] = dataset[\"raster\"][mask]\n",
    "                dataloader = init_dataloader(\n",
    "                    dataset[\"wvs\"], dataset[\"raster\"], batch_size, normalize=False\n",
    "                )\n",
    "            nss = NSS(\n",
    "                input_size=dataset[\"wvs\"].shape[1],\n",
    "                net_size=[120, 10],\n",
    "                threshold=0.03,\n",
    "                gamma=0.05,\n",
    "                lr=0.07,\n",
    "                bit_width=2,\n",
    "                seed=s,\n",
    "            )\n",
    "            nss_out, n_spikes = nss.fit_transform(dataloader)\n",
    "            sorted_spikes = np.argmax(nss_out, axis=1).astype(\n",
    "                int\n",
    "            )  # select most active neuron\n",
    "            packet_size = 200\n",
    "            spike_processed, fscore_nss_packet = compute_fscore_evolution(\n",
    "                sorted_spikes, dataset, packet_size\n",
    "            )\n",
    "            res[ds][f\"w/ FP\" if fps else \"w/o FP\"].append(fscore_nss_packet)\n",
    "            print(f\"f1s : {np.mean(np.mean(fscore_nss_packet, axis=0)[-5:])}\")\n",
    "\n",
    "# # save res dict to pickle\n",
    "# import pickle\n",
    "# with open(\"nss_robustness_false_positive.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(res, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute mean and std\n",
    "# load results\n",
    "with open(\"nss_robustness_false_positive.pkl\", \"rb\") as f:\n",
    "    res = pickle.load(f)\n",
    "th = 5\n",
    "fig, ax = plt.subplots()\n",
    "x_pos = -1\n",
    "for ds in datasets:\n",
    "    x_pos += 1\n",
    "    for fps in res[ds].keys():\n",
    "        f1_score = np.mean(np.mean(np.array(res[ds][fps]), axis=1)[:, -th:], axis=1)\n",
    "        mean, std = np.mean(f1_score), np.std(f1_score)\n",
    "        print(f\"{ds} {fps} mean: {mean:.3f} std: {std:.3f}\")\n",
    "        int_95 = 1.96 * std / np.sqrt(len(res[ds][fps]))\n",
    "\n",
    "        x_pos_b = x_pos + 0.1 if fps == \"w/ FP\" else x_pos - 0.1\n",
    "        color = \"blue\" if fps == \"w/ FP\" else \"orange\"\n",
    "        ax.errorbar(\n",
    "            x_pos_b,\n",
    "            mean,\n",
    "            yerr=int_95,\n",
    "            label=fps,\n",
    "            fmt=\"o\",\n",
    "            color=color,\n",
    "            capsize=5,\n",
    "            elinewidth=2,\n",
    "            markersize=8,\n",
    "        )\n",
    "ax.set_xticks(range(len(datasets)))\n",
    "ax.set_xticklabels(datasets)\n",
    "ax.yaxis.set_minor_locator(plt.MultipleLocator(0.01))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.05))\n",
    "ax.set_ylim(0.6, 0.89)\n",
    "ax.tick_params(axis=\"y\", which=\"major\", labelsize=12)\n",
    "ax.tick_params(axis=\"x\", labelsize=12)\n",
    "ax.set_ylabel(\"$F_{1}$ score\", fontsize=12)\n",
    "ax.set_xlabel(\"Dataset\", fontsize=12)\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "handles = [handles[0], handles[1]]\n",
    "labels = [labels[0], labels[1]]\n",
    "ax.legend(handles, labels, loc=\"best\", fontsize=12, edgecolor=\"w\")\n",
    "ax.spines[[\"top\", \"right\"]].set_visible(False)\n",
    "ax.yaxis.grid(True, which=\"major\", linestyle=\"--\", alpha=0.4)\n",
    "ax.yaxis.grid(True, which=\"minor\", linestyle=\"--\", alpha=0.2)\n",
    "# plt.savefig(\"../figures/figsupp4_nss_robustness_false_positive.svg\", dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([0.825 - 0.818, 0.741 - 0.734, 0.848 - 0.831, 0.752 - 0.748]) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"TS1\", \"TS2\", \"TS3\", \"TS4\"]\n",
    "conf_matrices = {\"TS1\": {}, \"TS2\": {}, \"TS3\": {}, \"TS4\": {}}\n",
    "for ds in datasets:\n",
    "    ds_file = data_path / f\"{ds}.h5\"\n",
    "    dataset, dataloader = build_dataloader(ds_file)\n",
    "    labels_peaks = compute_detection_performance(dataset)[\"labels_peaks\"]\n",
    "    snr = dataset[\"snr\"]\n",
    "    conf_matrices[ds][\"snr\"] = np.round(np.sort(snr), 1)\n",
    "    snr_perm = np.append(np.argsort(snr), 5)\n",
    "\n",
    "    print(f\"### dataset {ds} ###\")\n",
    "    print(f\"SNR : {snr}\")\n",
    "    nss = NSS(\n",
    "        input_size=dataset[\"wvs\"].shape[1],\n",
    "        net_size=[120, 10],\n",
    "        threshold=0.03,\n",
    "        gamma=0.05,\n",
    "        lr=0.07,\n",
    "        bit_width=2,\n",
    "        seed=0xC0FFE,\n",
    "    )\n",
    "    nss_out, n_spikes = nss.fit_transform(dataloader)\n",
    "    sorted_spikes = np.argmax(nss_out, axis=1).astype(int)  # select most active neuron\n",
    "    packet_size = 200\n",
    "    spike_processed, fscore_nss_packet = compute_fscore_evolution(\n",
    "        sorted_spikes, dataset, packet_size\n",
    "    )\n",
    "\n",
    "    ## Mask GT raster and detected raster to select last minute of recording\n",
    "    th_timing = 60\n",
    "    gtr = dataset[\"gt_raster\"]\n",
    "    unique, counts = np.unique(gtr[1], return_counts=True)\n",
    "    nneurons = len(np.unique(gtr[1]))\n",
    "    fs = dataset[\"fs\"]\n",
    "    spike_timing = dataset[\"raster\"]\n",
    "    sti = spike_timing[spike_timing >= spike_timing[-1] - th_timing * fs][0]\n",
    "    mask_pred = spike_timing >= sti\n",
    "    mask_gtr = gtr[0] >= sti\n",
    "\n",
    "    # Compute Confusion Matrix\n",
    "    gtsort_comp = SortingMetrics(\n",
    "        sorted_spikes[mask_pred],\n",
    "        spike_timing[mask_pred],\n",
    "        gtr[:, mask_gtr],\n",
    "        fs,\n",
    "        delta_time=2,\n",
    "    )\n",
    "    _ = gtsort_comp.get_sorting_perf(match_mode=\"hungarian\")\n",
    "    print(\n",
    "        f\" F1-score : {gtsort_comp.get_fscore()} \\ mean : {np.mean(gtsort_comp.get_fscore())}\"\n",
    "    )\n",
    "\n",
    "    confusion_matrix = gtsort_comp.sorting_perf.get_confusion_matrix()\n",
    "    matching_unit = gtsort_comp.sorting_perf.best_match_12.to_numpy()\n",
    "\n",
    "    ## Fill the rest of the confusion matrix\n",
    "    sorted_spikes_m = np.copy(sorted_spikes[mask_pred])\n",
    "    labels_peaks_m = np.copy(labels_peaks[mask_pred])\n",
    "\n",
    "    for true_i in range(5):\n",
    "        sorted_spikes_m_ui = sorted_spikes_m[labels_peaks_m == true_i]\n",
    "        for pred_j in [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]:\n",
    "            if (pred_j in matching_unit) and (\n",
    "                true_i == np.where(matching_unit == pred_j)[0]\n",
    "            ):\n",
    "                continue\n",
    "            confusion_matrix.loc[true_i, pred_j] = np.sum(sorted_spikes_m_ui == pred_j)\n",
    "\n",
    "    # conf_m = confusion_matrix.iloc[snr_perm, :]\n",
    "    # conf_m.index = np.arange(5).astype(str).tolist() + [\"FP\"]\n",
    "    # col_perm = np.append(np.argsort(np.array(conf_m.columns)[:-1]), 10)\n",
    "    # conf_m = conf_m.iloc[:, col_perm]\n",
    "    # conf_m.columns = np.arange(10).astype(str).tolist() + [\"FN\"]\n",
    "    conf_matrices[ds][\"conf_mat\"] = confusion_matrix\n",
    "\n",
    "# save conf_matrices to pickle\n",
    "with open(\"nss_confusion_matrices.pkl\", \"wb\") as f:\n",
    "    pickle.dump(conf_matrices, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion matrix\n",
    "# load conf_matrices\n",
    "# with open(\"nss_confusion_matrices.pkl\", \"rb\") as f:\n",
    "#     conf_matrices = pickle.load(f)\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 7), dpi=100, layout=\"constrained\")\n",
    "axs = axs.flatten()\n",
    "for ax, ds in zip(axs, conf_matrices.keys()):\n",
    "    # plot with color bar, one for all the plots\n",
    "    plot_conf = sns.heatmap(\n",
    "        conf_matrices[ds][\"conf_mat\"].astype(int),\n",
    "        # annot=True,\n",
    "        # fmt=\"d\",\n",
    "        cmap=\"Greys\",\n",
    "        cbar=False,\n",
    "        linewidths=1,\n",
    "        linecolor=\"k\",\n",
    "        # annot_kws={\"size\": 11},\n",
    "        ax=ax,\n",
    "        vmin=0,\n",
    "        vmax=conf_matrices[ds][\"conf_mat\"].max().max(),\n",
    "    )\n",
    "    # add grid for major ticks only\n",
    "    # ax.grid(which=\"major\", color=\"black\", linestyle=\"-\", linewidth=0.5)\n",
    "    if ax == axs[-1]:\n",
    "        cbar = plt.colorbar(plot_conf.collections[0], ax=ax, fraction=0.046, pad=0.04)\n",
    "        cbar.ax.tick_params(labelsize=11)\n",
    "        cbar.set_label(\"Number of Spikes\", fontsize=12)\n",
    "\n",
    "    if ds == \"TS1\" or ds == \"TS3\":\n",
    "        ax.set_ylabel(\"Ground Truth Unit Index \\n (True Label)\", fontsize=12)\n",
    "    elif ds == \"TS2\" or ds == \"TS4\":\n",
    "        ax.set_yticks([])\n",
    "    ax.set_title(\n",
    "        f\"{ds} \\n Units SNR: {conf_matrices[ds]['snr'][0]:.1f}, {conf_matrices[ds]['snr'][1]:.1f}, {conf_matrices[ds]['snr'][2]:.1f}, {conf_matrices[ds]['snr'][3]:.1f}, {conf_matrices[ds]['snr'][4]:.1f}\",\n",
    "        fontsize=11,\n",
    "    )\n",
    "    ax.set_yticklabels(ax.get_yticklabels(), rotation=0, fontsize=11)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), fontsize=11)\n",
    "plt.savefig(f\"nss_confusion_matrix.svg\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lca_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
